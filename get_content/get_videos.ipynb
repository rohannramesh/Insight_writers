{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from lxml.html import fromstring\n",
    "from collections import Counter\n",
    "from requests.packages.urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests import Session, exceptions\n",
    "import sys\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from newspaper import Article\n",
    "import json\n",
    "from string import digits\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter \n",
    "from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes word based on it's parts of speech\n",
    "from nltk.corpus import stopwords \n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import cmudict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from langdetect import detect\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "sys.path.append(\"/Users/rohanramesh/Documents/GitHub/Insight_writers/lib/\")\n",
    "from text_processing import ProcessArticle as pa\n",
    "import suggestions as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection with database\n",
    "client = MongoClient()\n",
    "mydb = client[\"testinsightdb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab config file\n",
    "# keys[0] apple, keys[1] listen notes\n",
    "path_teams = '/Users/rohanramesh/Documents/Insight/data_bball_writers/config.txt'\n",
    "with open(path_teams, 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "keys = [i.rstrip() for i in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_KEY = keys[0]\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_id(q, max_results,token, order=\"relevance\", \n",
    "                 location=None, location_radius=None):\n",
    "\n",
    "    search_response = youtube.search().list(\n",
    "        q=q, type=\"video\", pageToken=token,part=\"id,snippet\", \n",
    "        maxResults=max_results, location=location, \n",
    "        locationRadius=location_radius, safeSearch = 'strict').execute()\n",
    "    videoId = []\n",
    "    title = []\n",
    "    description = []\n",
    "    statistics = []\n",
    "    embed_url = []\n",
    "    tok = search_response['nextPageToken']\n",
    "\n",
    "    for search_result in search_response.get(\"items\", []):\n",
    "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "            title.append(search_result['snippet']['title'])\n",
    "            videoId.append(search_result['id']['videoId'])\n",
    "            response = youtube.videos().list(part='statistics, snippet, player', \n",
    "                                 id=search_result['id']['videoId']\n",
    "                                ).execute()\n",
    "            description.append(response['items'][0]['snippet']['description'])\n",
    "            statistics.append(response['items'][0]['statistics'])\n",
    "            embed_url.append(response['items'][0]['player']['embedHtml'])\n",
    "            \n",
    "    ydict = {'title':title,'videoId':videoId,\n",
    "              'description':description,'stats':statistics,\n",
    "             'embed_url':embed_url}\n",
    "    return ydict, tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vals = 'Lebron lakers'\n",
    "a,b = get_video_id(search_vals, 1, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spacy and load team names for named entity recognition\n",
    "nlp = en_core_web_sm.load()\n",
    "# team names\n",
    "path_teams = '/Users/rohanramesh/Documents/GitHub/Insight_writers/DashApp/assets/nba-teams.txt'\n",
    "with open(path_teams, 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "team_names = [i.rstrip() for i in tmp]\n",
    "\n",
    "def get_search_terms(string=None, team_names=None):\n",
    "    doc = nlp(string)\n",
    "    a = [i.text for i in doc.ents if (i.label_ == 'PERSON') | (i.label_ == 'GPE')]\n",
    "    # iterate through each word in string and compare to team_names if any hits then keep\n",
    "    if team_names is not None:\n",
    "        tokens = word_tokenize(string)\n",
    "        for i in tokens:\n",
    "            if not difflib.get_close_matches(i,team_names, n=1):\n",
    "                continue\n",
    "            else:\n",
    "                m = difflib.get_close_matches(i,team_names, n=1)[0]\n",
    "            a.append(m)\n",
    "    # now turn to normal string\n",
    "    input_search = ['']\n",
    "    for i in a:\n",
    "        input_search[0] = input_search[0] + i + ' '\n",
    "    return input_search[0]\n",
    "\n",
    "def parse_iframe_html(input_vec):\n",
    "    output_url = []\n",
    "    for i in input_vec:\n",
    "        idx = i.index('src=\"')\n",
    "        tmpR = i[idx+5:]\n",
    "        idx2 = tmpR.index('\"')\n",
    "        output_url.append(tmpR[:idx2])\n",
    "    return output_url\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value1 = 'https://theundefeated.com/features/how-jr-smith-cleveland-cavaliers-bounced-back-from-nba-finals/'\n",
    "article, lem_txt = s.grab_article(input_value1)\n",
    "search_string = get_search_terms(article.title, team_names)\n",
    "youtube_ouput, b = get_video_id(search_string, 3, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for old word2vec model\n",
    "kv = gensim.models.KeyedVectors.load(\n",
    "    \"/Users/rohanramesh/Documents/Insight/data_bball_writers/word2vec_model_kv.kv\", mmap='r')\n",
    "# load word2vec df for comparisons\n",
    "w2v_df2 = pd.read_pickle('/Users/rohanramesh/Documents/Insight/data_bball_writers/word2vec_trained.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to vector from descriptions\n",
    "vec_article = s.get_vector_from_w2v_model(kv, lem_txt)\n",
    "# for title\n",
    "tmp = s.lemstr(gensim.utils.simple_preprocess(youtube_ouput['title'][0]))\n",
    "vec_title = s.get_vector_from_w2v_model(kv, tmp)\n",
    "tmp = s.lemstr(gensim.utils.simple_preprocess(youtube_ouput['description'][0]))\n",
    "vec_description = s.get_vector_from_w2v_model(kv, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe for writer feature space\n",
    "writer_features = pd.read_pickle('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_features_USE.pickle')\n",
    "\n",
    "\n",
    "# load word2vec model\n",
    "kv = gensim.models.KeyedVectors.load(\n",
    "    \"/Users/rohanramesh/Documents/Insight/data_bball_writers/word2vec_model_kv.kv\", mmap='r')\n",
    "# load word2vec df for comparisons\n",
    "w2v_df = pd.read_pickle('/Users/rohanramesh/Documents/Insight/data_bball_writers/word2vec_trained.pickle')\n",
    "\n",
    "# client = MongoClient()\n",
    "# mydb = client[\"testinsightdb\"]\n",
    "\n",
    "# for logo\n",
    "# image_filename = '/Users/rohanramesh/Documents/GitHub/Insight_writers/DashApp/assets/favicon.ico' # replace with your own image\n",
    "# encoded_image = base64.b64encode(open(image_filename, 'rb').read())\n",
    "\n",
    "# for youtube suggestions\n",
    "# team names\n",
    "path_teams = '/Users/rohanramesh/Documents/GitHub/Insight_writers/DashApp/assets/nba-teams.txt'\n",
    "with open(path_teams, 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "team_names = [i.rstrip() for i in tmp]\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# config file\n",
    "path_teams = '/Users/rohanramesh/Documents/Insight/data_bball_writers/config.txt'\n",
    "with open(path_teams, 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "keys = [i.rstrip() for i in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a4350c9fd58c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# url = 'http://www.espn.com/nba/story/_/id/24796800/los-angeles-lakers-hold-first-practice-lebron-james'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://www.espn.com/fantasy/basketball/story/_/id/24731308/fantasy-basketball-fantasy-basketball-sleepers-breakouts-busts-2018-19'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlem_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mauthor_sugg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgive_suggestion_featurespace_single_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxtstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0marticle_sugg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommend_article_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlem_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlem_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "# url = 'http://www.espn.com/nba/story/_/id/24796800/los-angeles-lakers-hold-first-practice-lebron-james'\n",
    "url = 'http://www.espn.com/fantasy/basketball/story/_/id/24731308/fantasy-basketball-fantasy-basketball-sleepers-breakouts-busts-2018-19'\n",
    "article, lem_txt = s.grab_article(url)\n",
    "author_sugg = s.give_suggestion_featurespace_single_article(writer_features, txtstr=article.text)\n",
    "article_sugg = s.recommend_article_content(kv, w2v_df, lem_text=lem_txt)\n",
    "search_string = s.get_search_terms(article.title, team_names)\n",
    "print(search_string)\n",
    "# youtube_ouput, b = get_video_id(search_string, 3, None)\n",
    "# urls_to_pass = s.parse_iframe_html(embed_url) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4927339553833008\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.espn.com/fantasy/basketball/story/_/id/24731308/fantasy-basketball-fantasy-basketball-sleepers-breakouts-busts-2018-19'\n",
    "t = time.time()\n",
    "a = Article(url)\n",
    "a.download()\n",
    "a.parse()\n",
    "a.title\n",
    "elapsed = time.time() - t\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.45"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.49*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
