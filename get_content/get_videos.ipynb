{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from lxml.html import fromstring\n",
    "from collections import Counter\n",
    "from requests.packages.urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests import Session, exceptions\n",
    "import sys\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from newspaper import Article\n",
    "import json\n",
    "from string import digits\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter \n",
    "from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes word based on it's parts of speech\n",
    "from nltk.corpus import stopwords \n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import cmudict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from langdetect import detect\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "sys.path.append(\"/Users/rohanramesh/Documents/GitHub/Insight_writers/lib/\")\n",
    "from text_processing import ProcessArticle as pa\n",
    "import suggestions as s\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube video exploration\n",
    "Write test code to perform named entity recognition on the title of articles and test using the Youtube api to search for videos to embed on webpage. Write the functions that will be within my web app for searching and parsing youtube content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection with mongo database\n",
    "client = MongoClient()\n",
    "mydb = client[\"testinsightdb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab config file to get the api keys for youtube\n",
    "# keys[0] apple, keys[1] listen notes\n",
    "path_teams = '/Users/rohanramesh/Documents/Insight/data_bball_writers/config.txt'\n",
    "with open(path_teams, 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "keys = [i.rstrip() for i in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up connection to communicate with youtube to search for videos\n",
    "DEVELOPER_KEY = keys[0]\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to be able to match team names with place and this fails with most entity recognition so \n",
    "# incorporating text file with all team names\n",
    "# initialize spacy and load team names for named entity recognition\n",
    "nlp = en_core_web_sm.load()\n",
    "# team names txt file\n",
    "path_teams = '/Users/rohanramesh/Documents/GitHub/Insight_writers/DashApp/assets/nba-teams.txt'\n",
    "with open(path_teams, 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "team_names = [i.rstrip() for i in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_id(q, max_results,token, order=\"relevance\", \n",
    "                 location=None, location_radius=None):\n",
    "    \"\"\"\n",
    "    enter in search terms and get out metadate for youtube vido as well as the links to embed videos\n",
    "    :param q: search terms\n",
    "    :param max_results: max number of results back you want\n",
    "    :param token: page search number set to none if want limited returns\n",
    "    :param order: how to sort output (temporal vs relevance)\n",
    "    :return: ydict: dictionary with metadata and embedding url\n",
    "    :return: tok: token for next search\n",
    "    \"\"\"\n",
    "    search_response = youtube.search().list(\n",
    "        q=q, type=\"video\", pageToken=token,part=\"id,snippet\", \n",
    "        maxResults=max_results, location=location, \n",
    "        locationRadius=location_radius, safeSearch = 'strict').execute()\n",
    "    videoId = []\n",
    "    title = []\n",
    "    description = []\n",
    "    statistics = []\n",
    "    embed_url = []\n",
    "    tok = search_response['nextPageToken']\n",
    "\n",
    "    for search_result in search_response.get(\"items\", []):\n",
    "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "            title.append(search_result['snippet']['title'])\n",
    "            videoId.append(search_result['id']['videoId'])\n",
    "            response = youtube.videos().list(part='statistics, snippet, player', \n",
    "                                 id=search_result['id']['videoId']\n",
    "                                ).execute()\n",
    "            description.append(response['items'][0]['snippet']['description'])\n",
    "            statistics.append(response['items'][0]['statistics'])\n",
    "            embed_url.append(response['items'][0]['player']['embedHtml'])\n",
    "            \n",
    "    ydict = {'title':title,'videoId':videoId,\n",
    "              'description':description,'stats':statistics,\n",
    "             'embed_url':embed_url}\n",
    "    return ydict, tok\n",
    "\n",
    "def get_search_terms(string=None, team_names=None):\n",
    "    \"\"\"\n",
    "    Do named entity recognition of title of article to enter into youtube search and\n",
    "    incorporate in list of team names\n",
    "    :param string: Title of article\n",
    "    :param team_names: All team names\n",
    "    :return: input_search: the string to enter to youtube for searching\n",
    "    \"\"\"\n",
    "    doc = nlp(string)\n",
    "    a = [i.text for i in doc.ents if (i.label_ == 'PERSON') | (i.label_ == 'GPE')]\n",
    "    # iterate through each word in string and compare to team_names if any hits then keep\n",
    "    if team_names is not None:\n",
    "        tokens = word_tokenize(string)\n",
    "        for i in tokens:\n",
    "            if not difflib.get_close_matches(i,team_names, n=1):\n",
    "                continue\n",
    "            else:\n",
    "                m = difflib.get_close_matches(i,team_names, n=1)[0]\n",
    "            a.append(m)\n",
    "    # now turn to normal string\n",
    "    input_search = ['']\n",
    "    for i in a:\n",
    "        input_search[0] = input_search[0] + i + ' '\n",
    "    return input_search[0]\n",
    "\n",
    "def parse_iframe_html(input_vec):\n",
    "    \"\"\"\n",
    "    parse the embedded url to pass to my web app\n",
    "    incorporate in list of team names\n",
    "    :param input_vec: url to be embedded from youtube\n",
    "    :return: output_url: the exact string to embed\n",
    "    \"\"\"\n",
    "    output_url = []\n",
    "    for i in input_vec:\n",
    "        idx = i.index('src=\"')\n",
    "        tmpR = i[idx+5:]\n",
    "        idx2 = tmpR.index('\"')\n",
    "        output_url.append(tmpR[:idx2])\n",
    "    return output_url\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
