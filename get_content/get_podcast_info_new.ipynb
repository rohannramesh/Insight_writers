{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from lxml.html import fromstring\n",
    "from collections import Counter\n",
    "from requests.packages.urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests import Session, exceptions\n",
    "import sys\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sys.path.append(\"/Users/rohanramesh/Documents/GitHub/Insight_writers/Dash_to_server/\")\n",
    "from text_processing import ProcessArticle as pa\n",
    "import suggestions as s\n",
    "import difflib\n",
    "from scipy.stats import wilcoxon, sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab Podcast information\n",
    "The goal of this is to validate my writer style recommendations by showing that writers with a similar writing style are more likely to do a podcast together than chance. To do this I will use the listennotes api to get the metdata about the 50 most recent podcasts for each writer and check to see if the last name of another author was in the same podcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/Attempt3_mr_scrape.pickle', 'rb') as handle:\n",
    "    scrapevar = pickle.load(handle)\n",
    "\n",
    "# load writer df\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_df.pickle', 'rb') as handle:\n",
    "    writer_df = pickle.load(handle)\n",
    "\n",
    "# load writer features\n",
    "writer_features = pd.read_pickle('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_features_USE.pickle')\n",
    "\n",
    "# load podcasts - IF ALREADY SCRAPED\n",
    "top_podcasts = pd.read_pickle('/Users/rohanramesh/Documents/Insight/data_bball_writers/top50podcasts.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab config file that has the apikeys for listennotes\n",
    "# keys[0] apple, keys[1] listen notes\n",
    "path_teams = '/Users/rohanramesh/Documents/Insight/data_bball_writers/config.txt'\n",
    "with open(path_teams, 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "keys = [i.rstrip() for i in tmp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull necessary info from writer and article database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://rohanramesh@localhost/writer_feature_db\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# grab the relevant information from the database of writers and articles\n",
    "dbname = 'writer_feature_db'\n",
    "# dbname = 'writer_db'\n",
    "username = 'rohanramesh' # change this to your username\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to load entire db into pandas\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM writer_feature;\n",
    "\"\"\"\n",
    "writer_feature_df = pd.read_sql_query(sql_query,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT AVG(\"n_words\") AS \"avg_n_words\",\n",
    "         AVG(\"neg_sent\") AS \"avg_neg_sent\",\n",
    "         AVG(\"neu_sent\") AS \"avg_neu_sent\",\n",
    "         AVG(\"pos_sent\") AS \"avg_pos_sent\",\n",
    "         AVG(\"neg_sent_var\") AS \"avg_neg_sent_var\",\n",
    "         AVG(\"neu_sent_var\") AS \"avg_neu_sent_var\",\n",
    "         AVG(\"pos_sent_var\") AS \"avg_pos_sent_var\",\n",
    "        AVG(\"n_sentences\") AS \"avg_n_sentences\",\n",
    "        AVG(\"n_wordspersentence\") AS \"avg_n_wordspersentence\",\n",
    "        AVG(\"n_wordspersent_variability\") AS \"avg_n_wordspersent_variability\",\n",
    "        AVG(\"wordlength\") AS \"avg_wordlength\",\n",
    "        AVG(\"wordlength_var\") AS \"avg_wordlength_var\",\n",
    "        AVG(\"wordlength_skew\") AS \"avg_wordlength_skew\",\n",
    "        \"author_list\"\n",
    "FROM writer_feature\n",
    "WHERE \"n_words\" > 100\n",
    "GROUP BY \"author_list\"\n",
    "HAVING COUNT(*) > 25 \n",
    "ORDER BY AVG(\"n_words\") DESC\n",
    "\"\"\"\n",
    "writer_feature_subsection = pd.read_sql_query(sql_query,con)\n",
    "writer_feature_subsection.head(10)\n",
    "writer_feature_subsection.replace([np.inf, -np.inf], np.nan)\n",
    "writer_feature_subsection = writer_feature_subsection.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an author list with full names not just website label names\n",
    "author_webnames = writer_feature_subsection['author_list']\n",
    "author_list = []\n",
    "for i in author_webnames:\n",
    "    idx = writer_df['website_name'] == i\n",
    "    author_list.append(writer_df['Idea Text'][np.where(idx)[0]].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where I actually use api to grab top 50 podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually grab podcast information using the listen notes api\n",
    "top_podcasts = {}\n",
    "# iterate through all authors\n",
    "for curr_author in author_list:\n",
    "    top_podcasts[curr_author] = []\n",
    "    # break up into first and last name for formatting on their website purposes\n",
    "    r = word_tokenize(curr_author)\n",
    "    # get 10 hits per page so grab the top 50\n",
    "    page_idx = 0\n",
    "    while page_idx < 50:\n",
    "        # url in their formatting, searching for a given author, not sorting by data, sorting by relevance\n",
    "        url = (\"https://listennotes.p.mashape.com/api\"\n",
    "            \"/v1/search?offset={}&q={}+{}&sort_by_date=0&type=episode\".format(page_idx,r[0], r[1]))\n",
    "        response = requests.get(url,\n",
    "          headers={\n",
    "            \"X-Mashape-Key\": keys[1],\n",
    "            \"Accept\": \"application/json\"\n",
    "          }\n",
    "        )\n",
    "        a = response.content\n",
    "        b = json.loads(a)\n",
    "        top_podcasts[curr_author].append(b)\n",
    "        page_idx += 10\n",
    "        time.sleep(random.uniform(0.5,2))\n",
    "        print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each podcast search through all descriptions for other authors name and if their is a comention\n",
    "# then set that match to 1 in the author mentions matrix (authors x authors)\n",
    "author_mentions = np.zeros([len(author_list), len(author_list)])\n",
    "for i in range(0,len(author_list)):\n",
    "    curr_author = author_list[i]\n",
    "    # only have 4 pages bc going through top 50 podcasts\n",
    "    for curr_page in range(0,4):\n",
    "        # have up to 10 hits per page and searching in the results section\n",
    "        for curr_pod in range(0,np.min([9,len(top_podcasts[curr_author][curr_page]['results'])])):\n",
    "            # this is the info from this podcast\n",
    "            curr_info = top_podcasts[curr_author][curr_page]['results'][curr_pod]\n",
    "            podcast_keys = curr_info.keys()\n",
    "            matches = []\n",
    "            # iterate through all metadata information and search for every possible last name\n",
    "            for curr_key in podcast_keys:\n",
    "                for j in range(0,len(author_list)):\n",
    "                    curr_authorcomp = author_list[j]\n",
    "                    # bc of silly formatting for Jason Concepcion\n",
    "                    if curr_authorcomp == 'Jason Concepcion/netw3rk':\n",
    "                        curr_authorcomp = 'Jason Concepcion'\n",
    "                    search_terms = curr_authorcomp\n",
    "                    if isinstance(curr_info[curr_key], str):\n",
    "                        last_name = word_tokenize(search_terms)[1]\n",
    "                        # if last name of other author is in the metadata then set\n",
    "                        # author_mentions[author1,author2] = 1 \n",
    "                        # so can find co-mentions later\n",
    "                        if last_name in curr_info[curr_key]:\n",
    "                            author_mentions[i,j] = 1\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For writer co-mentions I don't care if Zach Lowe is on Lee Jenkins' podcast or vice versa so will take a mention in either direction as evidence they have done a podcast together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris Ballard\n",
      "Justin Jett\n"
     ]
    }
   ],
   "source": [
    "# iterate each author and see how many times match in podcasts in either direction\n",
    "website_author_names = writer_df['website_name'].tolist()\n",
    "matched_fraction = []\n",
    "nonmatch_fraction = []\n",
    "for ii in range(0,len(author_list)):\n",
    "    # if writer has one or fewer hits (ie. doesn't have a podcast) then ignore\n",
    "    if np.sum(author_mentions[ii,:]) <= 1:\n",
    "        print(author_list[ii])\n",
    "        continue\n",
    "    curr_author = author_list[ii]\n",
    "    website_name = s.match_author_names(writer_df, curr_author)\n",
    "    # this is the writers that are suggested based off the writing style of current writer\n",
    "    author_sugg = s.give_author_suggestion_from_author(writer_features, curr_author)\n",
    "    author_sugg_list = author_sugg['Author_wn'].tolist()\n",
    "    matched_authors = author_sugg_list[0:3] # already removed same author - looking at top 3 hits\n",
    "    # these are all of the nonmatched authors\n",
    "    nonmatched_authors = [i for i in website_author_names if (i not in matched_authors)\n",
    "                         and (i != curr_author)]\n",
    "    # get n and fraction for matched authors - ie how many out of top were on a podcast together\n",
    "    n_match = []\n",
    "    for i in matched_authors:\n",
    "        idx1 = author_list.index(curr_author)\n",
    "        idx2 = author_list.index(match_author_names(writer_df, i))\n",
    "        val = np.max([author_mentions[idx1,idx2], author_mentions[idx2,idx1]])\n",
    "        n_match.append(val)\n",
    "    matched_fraction.append(np.sum(n_match)/len(n_match))\n",
    "    # get n and fraction for nonmatched\n",
    "    n_nonmatch = []\n",
    "    for i in nonmatched_authors:\n",
    "        if match_author_names(writer_df, i) in author_list:\n",
    "            idx1 = author_list.index(curr_author)  \n",
    "            idx2 = author_list.index(match_author_names(writer_df, i))\n",
    "            val = np.max([author_mentions[idx1,idx2], author_mentions[idx2,idx1]])\n",
    "            n_nonmatch.append(val)\n",
    "    nonmatch_fraction.append(np.sum(n_nonmatch)/len(n_nonmatch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot distributions, look at means, do rank-sum test\n",
    "plt.hist(nonmatch_fraction)\n",
    "plt.show()\n",
    "plt.hist(matched_fraction)\n",
    "plt.show()\n",
    "print(np.mean(matched_fraction))\n",
    "print(np.mean(nonmatch_fraction))\n",
    "print(np.mean(matched_fraction)/np.mean(nonmatch_fraction))\n",
    "a,pval = wilcoxon(matched_fraction, nonmatch_fraction)\n",
    "pval\n",
    "# plot barplot for matched vs random\n",
    "plt.bar(range(0,2),[np.mean(matched_fraction)*100, np.mean(nonmatch_fraction)*100], 0.75, \n",
    "        yerr=[sem(matched_fraction)*100, sem(nonmatch_fraction)*100], tick_label= ['Suggested', 'Random'])\n",
    "plt.ylabel('Percent of writers to do podcasts together')\n",
    "plt.savefig('/Users/rohanramesh/Documents/Insight/Presentation_material/Figures/Podcast.eps', format='eps', dpi=1000)\n",
    "plt.show()\n",
    "# pval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
