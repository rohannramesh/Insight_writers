{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from lxml.html import fromstring\n",
    "from requests.packages.urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests import Session, exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get URL Links for writers from muckrack.com\n",
    "The goal of this notebook is to scrape muckrack.com for the url links for all nba writers. I am using muckrack.com as a way to get all relevant content for each NBA writer regardless of the platform hosting the article. Muckrack.com is a website for journalists and each writer has a page with a link and brief metadata about all of the most recent articles by a given writer. If the writer has a login to this website they can customize their username, otherwise the webpage for that writer is first_name-last_name. I manually went and searched all of the writers to be included in my database to make sure I had the right link to get the right articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if already have muckrack data\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_df.pickle', 'rb') as handle:\n",
    "    writer_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source of names of NBA writers - from NBA writer poll on twitter\n",
    "writer_filepath = '/Users/rohanramesh/Documents/Insight/data_bball_writers/writers.csv'\n",
    "writer_df = pd.read_csv(writer_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the writers that have personalized usernames that I manually had to find and update\n",
    "# make new column that has edited name for search and save this df\n",
    "curr_name = writer_df['Idea Text'][0]\n",
    "website_name = []\n",
    "for curr_name in writer_df['Idea Text']:\n",
    "    new_name = curr_name.lower().replace(' ','-')\n",
    "    new_name = new_name.lower().replace('.','')\n",
    "    if '/' in new_name:\n",
    "        idx = new_name.index('/')\n",
    "        new_name = new_name[0:idx]\n",
    "    if (new_name == 'bill-simmons') or (new_name == 'rob-mahoney'):\n",
    "        new_name = new_name.replace('-','') # bc multiple bill simmons\n",
    "    if (new_name == 'scott-howard-cooper'):\n",
    "        new_name = 'showardcooper'\n",
    "    if (curr_name == 'Zach Lowe'):\n",
    "        new_name = 'zachlowe_nba'\n",
    "    if (curr_name == 'Sam Amick'):\n",
    "        new_name = 'sam_amick'\n",
    "    if (curr_name == 'Jonathan Abrams'):\n",
    "        new_name = 'jpdabrams'\n",
    "    if (curr_name == 'Marc Spears'):\n",
    "        new_name = 'marc-j-spears'\n",
    "    if (curr_name == \"Kevin O'Connor\"):\n",
    "        new_name = 'kevin-p-oconnor'\n",
    "    if (curr_name == 'Marcus Thompson'):\n",
    "        new_name = 'thompsonscribe'\n",
    "    if (curr_name == 'Katie Nolan'):\n",
    "        new_name = 'meet-katie-nolan'\n",
    "    if (curr_name == 'Eric Pincus'):\n",
    "        new_name = 'ericpincus'\n",
    "    if (curr_name == 'Tim Kawakami'):\n",
    "        new_name = 'timkawakami'\n",
    "    if (curr_name == 'Josh Robbins'):\n",
    "        new_name = 'joshrobbins'\n",
    "    if (curr_name == \"Ian O'Connor\"):\n",
    "        new_name = 'ian-oconnor-1027184'\n",
    "    if (curr_name == \"Chris Broussard\"):\n",
    "        new_name = 'chris_broussard'\n",
    "    if (curr_name == \"Howard Beck\"):\n",
    "        new_name = 'howardbeck'\n",
    "    if (curr_name == \"Jeff Zillgitt\"):\n",
    "        new_name = 'jeffzillgitt'\n",
    "    if (curr_name == \"Jon Krawczynski\"):\n",
    "        new_name = 'apkrawczynski'\n",
    "    if (curr_name == \"Bob Ryan\"):\n",
    "        new_name = 'globebobryan'\n",
    "    if (curr_name == \"Ric Bucher\"):\n",
    "        new_name = 'ricbucher'\n",
    "    if (curr_name == \"Scott Cacciola\"):\n",
    "        new_name = 'scottcacciola'\n",
    "    if (curr_name == \"David Morrow\"):\n",
    "        new_name = 'david-a-morrow'\n",
    "    if (curr_name == \"Peter Vecsey\"):\n",
    "        new_name = 'petervecsey1'\n",
    "    if (curr_name == \"Jimmy Spencer\"):\n",
    "        new_name = 'jimmy-spencer-1'\n",
    "    if (curr_name == \"Seb Dumitru\"):\n",
    "        new_name = 'sebdumitru'\n",
    "    website_name.append(new_name)\n",
    "\n",
    "\n",
    "# website_name = [i.lower().replace(' ','-') for i in writer_df['Idea Text']]\n",
    "writer_df['website_name'] = website_name\n",
    "writer_df.head(100)\n",
    "# save writer df\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(writer_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping muckrack\n",
    "In the upcoming cell I scrape all of the top 20 pages worth of articles for each writer and store the url and metadata into a variable called scrapevar that I can save for later use to scrape the actual article content. I attempted this two ways:\n",
    "\n",
    "1. First I just randomize a wait time but use my IP address \n",
    "2. I grab other IP addresses and headers to avoid rate limiting issues\n",
    "\n",
    "While I got kicked off initially, I found by further randomizing my wait time I could use option one but I have included both sets of code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zachlowe_nba\n",
      "25\n",
      "adrian-wojnarowski\n",
      "25\n",
      "lee-jenkins\n",
      "25\n",
      "howardbeck\n",
      "25\n",
      "marc-stein\n",
      "25\n",
      "ethan-sherwood-strauss\n",
      "25\n",
      "jason-concepcion\n",
      "25\n",
      "kevin-arnovitz\n",
      "25\n",
      "tom-haberstroh\n",
      "25\n",
      "nate-duncan\n",
      "24\n",
      "zach-harper\n",
      "25\n",
      "brian-windhorst\n",
      "25\n",
      "sam_amick\n",
      "25\n",
      "david-aldridge\n",
      "25\n",
      "ramona-shelburne\n",
      "25\n",
      "jpdabrams\n",
      "25\n",
      "kevin-pelton\n",
      "25\n",
      "marc-j-spears\n",
      "25\n",
      "matt-moore\n",
      "25\n",
      "shams-charania\n",
      "25\n",
      "kevin-p-oconnor\n",
      "25\n",
      "chris-haynes\n",
      "25\n",
      "dave-mcmenamin\n",
      "25\n",
      "shea-serrano\n",
      "25\n",
      "ian-begley\n",
      "25\n",
      "rachel-nichols\n",
      "25\n",
      "ben-golliver\n",
      "25\n",
      "mike-prada\n",
      "25\n",
      "robmahoney\n",
      "25\n",
      "tim-macmahon\n",
      "25\n",
      "chris-herring\n",
      "25\n",
      "billsimmons\n",
      "25\n",
      "jonathan-tjarks\n",
      "25\n",
      "amin-elhassen\n",
      "0\n",
      "bobby-marks\n",
      "25\n",
      "danny-leroux\n",
      "25\n",
      "chris-mannix\n",
      "25\n",
      "dan-devine\n",
      "25\n",
      "michael-pina\n",
      "25\n",
      "thompsonscribe\n",
      "25\n",
      "ben-golliver\n",
      "25\n",
      "ben-falk\n",
      "25\n",
      "ian-levy\n",
      "25\n",
      "tim-bontemps\n",
      "25\n",
      "henry-abbott\n",
      "25\n",
      "scott-rafferty\n",
      "25\n",
      "ja-dubin\n",
      "0\n",
      "michael-lee\n",
      "25\n",
      "alex-kennedy\n",
      "25\n",
      "derek-bodner\n",
      "25\n",
      "tom-ziller\n",
      "25\n",
      "chris-ballard\n",
      "25\n",
      "josh-eberley\n",
      "25\n",
      "adi-joseph\n",
      "25\n",
      "adam-mares\n",
      "25\n",
      "sam-vecenie\n",
      "25\n",
      "meet-katie-nolan\n",
      "1\n",
      "andrew-sharp\n",
      "25\n",
      "david-thorpe\n",
      "25\n",
      "royce-webb\n",
      "1\n",
      "ericpincus\n",
      "25\n",
      "nick-sciria\n",
      "19\n",
      "paul-flannery\n",
      "25\n",
      "timkawakami\n",
      "25\n",
      "keith-smith\n",
      "25\n",
      "joshrobbins\n",
      "25\n",
      "jeff-zillgitt\n",
      "0\n",
      "jon-krawczynski\n",
      "0\n",
      "seerat-sohi\n",
      "25\n",
      "israel-gutierrez\n",
      "17\n",
      "ben-rohrbach\n",
      "25\n",
      "mika-honkasalo\n",
      "24\n",
      "ian-oconnor-1027184\n",
      "24\n",
      "kevin-ferrigan\n",
      "17\n",
      "chris-vernon\n",
      "25\n",
      "trevor-magnotti\n",
      "25\n",
      "mark-deeks\n",
      "25\n",
      "ken-berger\n",
      "25\n",
      "beckley-mason\n",
      "7\n",
      "bob-ryan\n",
      "1\n",
      "neil-paine\n",
      "25\n",
      "jeff-siegel\n",
      "25\n",
      "sam-amico\n",
      "25\n",
      "mason-ginsberg\n",
      "11\n",
      "britt-robson\n",
      "25\n",
      "holly-mackenzie\n",
      "25\n",
      "blake-murphy\n",
      "25\n",
      "sam-esfandiari\n",
      "0\n",
      "jade-hoye\n",
      "0\n",
      "michael-grange\n",
      "25\n",
      "ric-bucher\n",
      "0\n",
      "kevin-parrish\n",
      "25\n",
      "jared-zwerling\n",
      "25\n",
      "frank-isola\n",
      "25\n",
      "kacy-sager\n",
      "5\n",
      "austin-hutchinson\n",
      "25\n",
      "senthil-natarajan\n",
      "0\n",
      "kaileigh-brandt\n",
      "0\n",
      "trill-withers\n",
      "0\n",
      "krishna-narsu\n",
      "24\n",
      "chris-bernucca\n",
      "19\n",
      "chris-sheridan\n",
      "25\n",
      "scott-cacciola\n",
      "0\n",
      "albert-nahmad\n",
      "0\n",
      "showardcooper\n",
      "25\n",
      "james-holas\n",
      "25\n",
      "brady-klopfer\n",
      "25\n",
      "josh-eberley\n",
      "25\n",
      "rory-masterson\n",
      "25\n",
      "grant-afseth\n",
      "25\n",
      "rob-lopez\n",
      "25\n",
      "justin-willard\n",
      "25\n",
      "justin-hodges\n",
      "25\n",
      "christian-rivas\n",
      "25\n",
      "andrew-bernucca\n",
      "25\n",
      "steve-kyler\n",
      "25\n",
      "david-morrow\n",
      "0\n",
      "austin-green\n",
      "25\n",
      "mike-brady\n",
      "12\n",
      "peter-vecsey\n",
      "0\n",
      "rob-scott\n",
      "25\n",
      "paolo-uggetti\n",
      "25\n",
      "coley-mick\n",
      "1\n",
      "noah-torr\n",
      "25\n",
      "george-rowland\n",
      "0\n",
      "editor-dan\n",
      "0\n",
      "dennis-chambers\n",
      "25\n",
      "chris_broussard\n",
      "25\n",
      "justin-jett\n",
      "25\n",
      "jimmy-spencer\n",
      "2\n",
      "tevin-williams\n",
      "25\n",
      "ti-windisch\n",
      "25\n",
      "glory-okoli\n",
      "0\n",
      "aaron-bruski\n",
      "15\n",
      "paul-centopani\n",
      "25\n",
      "stephen-a-smith\n",
      "25\n",
      "andré-voigt\n",
      "1\n",
      "skip-bayless\n",
      "25\n",
      "seb-dumitru\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# preallocate the scrapevar dict\n",
    "scrapevar = {}\n",
    "cats_collect = {'blurbs', 'links', 'titles', 'sources', 'pubdates'}\n",
    "for i in cats_collect:\n",
    "    scrapevar[i] = {}\n",
    "\n",
    "# iterate through all writer names\n",
    "for curr_name in writer_df['website_name']:\n",
    "    print(curr_name)\n",
    "    # this is the formatting for the muckrack page and name\n",
    "    page_address = 'https://muckrack.com/%s/articles?page=' % curr_name\n",
    "    all_soups = []\n",
    "    all_blurbs = []\n",
    "    all_links = []\n",
    "    all_titles = []\n",
    "    all_sources = []\n",
    "    all_pubdates = []\n",
    "    npage = 1\n",
    "    n_pages_to_scrape = 20 # this is the number of pages to scrape for each writer (25 hits per page)\n",
    "    while npage <= n_pages_to_scrape:\n",
    "        # use requests to try and grab url if fail move on to next writer\n",
    "        try:\n",
    "            page = requests.get(page_address+str(npage), timeout=10)\n",
    "        except Exception:\n",
    "            npage = n_pages_to_scrape+10\n",
    "            continue\n",
    "        # using beautifulsoupt to parse the text for muckrack\n",
    "        curr_soup = BeautifulSoup(page.text)\n",
    "        time.sleep(random.uniform(0.5,2))\n",
    "        all_soups.append(curr_soup)\n",
    "        # now digging into metadata to get urls\n",
    "        allstories = curr_soup.findAll(\"div\", {\"class\": \"news-story-meta\"})\n",
    "        if not allstories: # jump to next writer if fails\n",
    "            npage = n_pages_to_scrape+10\n",
    "            break\n",
    "        # if should keep on searching till next page bc of an endless container\n",
    "        a = curr_soup.find(\"div\", {\"class\": \"endless_container\"})\n",
    "        if a is not None:\n",
    "            npage += 1\n",
    "        else:\n",
    "            npage = n_pages_to_scrape+10\n",
    "        for curr_story in allstories:\n",
    "            # save and append info\n",
    "            b = curr_story.find(\"a\", {\"target\": \"_blank\"})\n",
    "            all_blurbs.append(b.attrs['data-description'])\n",
    "            all_links.append(b.attrs['data-link'])\n",
    "            all_titles.append(b.attrs['data-title'])\n",
    "            all_sources.append(b.attrs['data-source'])\n",
    "            timelabel = curr_story.find(\"a\", {\"class\": \"timeago\"})\n",
    "            all_pubdates.append(timelabel.attrs['title'])\n",
    "    # put into scrapevar\n",
    "    scrapevar['blurbs'][curr_name] = all_blurbs\n",
    "    scrapevar['links'][curr_name] = all_links\n",
    "    scrapevar['titles'][curr_name] = all_titles\n",
    "    print(len(scrapevar['titles'][curr_name]))\n",
    "    scrapevar['sources'][curr_name] = all_sources\n",
    "    scrapevar['pubdates'][curr_name] = all_pubdates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions written to get around rate limits using proxies and different user agents\n",
    "# get_proxies():\n",
    "def get_proxies(nprox):\n",
    "    \"\"\"\n",
    "    grab from free-proxy-list.net to avoid rate limits\n",
    "    :param: nprox: number of proxies you want out\n",
    "    :return: list of proxies to use\n",
    "    \"\"\"\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    curr_soup = BeautifulSoup(response.text)\n",
    "    rand_ipadd = []\n",
    "    alllist = curr_soup.findAll(\"tr\")\n",
    "    # alllist[1].next.text\n",
    "    rvec = random.sample(range(1, 15), nprox)\n",
    "    for i in rvec:\n",
    "        rand_ipadd.append((alllist[i].next.text + \":\" + alllist[i].next.next.next.text))\n",
    "    return rand_ipadd\n",
    "\n",
    "def get_user_agents(nagents):\n",
    "    \"\"\"\n",
    "    grab n user agents to avoid rate limist\n",
    "    :param: nagents: number of agents you want out\n",
    "    :return: list of agents to use\n",
    "    \"\"\"\n",
    "    useragent_add = 'https://developers.whatismybrowser.com/useragents/explore/software_type_specific/web-browser/'\n",
    "    page = requests.get(useragent_add)\n",
    "    curr_soup = BeautifulSoup(page.text)\n",
    "    rand_useragents = []\n",
    "    alllist = curr_soup.findAll(\"td\", {\"class\": \"useragent\"})\n",
    "    rvec = random.sample(range(0, 25), nagents)\n",
    "    for i in rvec:\n",
    "        a = alllist[i].find(\"a\")\n",
    "        rand_useragents.append(a.text)\n",
    "    return rand_useragents\n",
    "\n",
    "def build_proxy_agent(proxy_list,user_agent_list):\n",
    "    \"\"\"\n",
    "    combine proxies and agents to build new header and userdict to scrape with\n",
    "    :param: proxy_list: proxy list generate from free-proxy-list\n",
    "    :param: user_agent_list: user agents from whatismybrowser\n",
    "    :return: proxyDict: Dict formatted for http and https\n",
    "    :return: header: randomly grab user agent from list and return\n",
    "    \"\"\"\n",
    "    proxy = random.choice(proxy_list)\n",
    "    http_proxy  = \"http://\" + proxy\n",
    "    https_proxy = \"https://\" + proxy\n",
    "\n",
    "    proxyDict = { \n",
    "                  \"http\"  : http_proxy, \n",
    "                  \"https\" : https_proxy\n",
    "                }\n",
    "    user_agent = random.choice(user_agent_list)\n",
    "    #Set the headers \n",
    "    headers = {'User-Agent': user_agent}\n",
    "    return proxyDict, headers\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    iteratively try with requests and proxies and user agents to download url\n",
    "    often need to do this bc might have issues with certain addresses\n",
    "    :return: session: the session for scraping\n",
    "    \"\"\"    \n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process but using proxies and user agents\n",
    "proxy_list = get_proxies(12)\n",
    "user_agent_list = get_user_agents(12)\n",
    "\n",
    "scrapevar = {}\n",
    "cats_collect = {'blurbs', 'links', 'titles', 'sources', 'pubdates'}\n",
    "for i in cats_collect:\n",
    "    scrapevar[i] = {}\n",
    "\n",
    "# iterate through all writer names\n",
    "for curr_name in writer_df['website_name']:\n",
    "    print(curr_name)\n",
    "    # this is the formatting for the muckrack page and name\n",
    "    page_address = 'https://muckrack.com/%s/articles?page=' % curr_name\n",
    "    all_soups = []\n",
    "    all_blurbs = []\n",
    "    all_links = []\n",
    "    all_titles = []\n",
    "    all_sources = []\n",
    "    all_pubdates = []\n",
    "    npage = 1\n",
    "    n_pages_to_scrape = 20 # this is the number of pages to scrape for each writer (25 hits per page)\n",
    "    while npage <= n_pages_to_scrape:\n",
    "        # use requests to try and grab url if fail move on to next writer\n",
    "        try:\n",
    "            page = requests_retry_session(retries=10).get(\n",
    "                page_address+str(npage), headers=headers, proxies=proxyDict, timeout=10)\n",
    "        except Exception:\n",
    "            npage = n_pages_to_scrape+10\n",
    "            continue\n",
    "        # using beautifulsoupt to parse the text for muckrack\n",
    "        curr_soup = BeautifulSoup(page.text)\n",
    "        time.sleep(random.uniform(0.5,2))\n",
    "        all_soups.append(curr_soup)\n",
    "        # now digging into metadata to get urls\n",
    "        allstories = curr_soup.findAll(\"div\", {\"class\": \"news-story-meta\"})\n",
    "        if not allstories: # jump to next writer if fails\n",
    "            npage = n_pages_to_scrape+10\n",
    "            break\n",
    "        # if should keep on searching till next page bc of an endless container\n",
    "        a = curr_soup.find(\"div\", {\"class\": \"endless_container\"})\n",
    "        if a is not None:\n",
    "            npage += 1\n",
    "        else:\n",
    "            npage = n_pages_to_scrape+10\n",
    "        for curr_story in allstories:\n",
    "            # save and append info\n",
    "            b = curr_story.find(\"a\", {\"target\": \"_blank\"})\n",
    "            all_blurbs.append(b.attrs['data-description'])\n",
    "            all_links.append(b.attrs['data-link'])\n",
    "            all_titles.append(b.attrs['data-title'])\n",
    "            all_sources.append(b.attrs['data-source'])\n",
    "            timelabel = curr_story.find(\"a\", {\"class\": \"timeago\"})\n",
    "            all_pubdates.append(timelabel.attrs['title'])\n",
    "    # put into scrapevar\n",
    "    scrapevar['blurbs'][curr_name] = all_blurbs\n",
    "    scrapevar['links'][curr_name] = all_links\n",
    "    scrapevar['titles'][curr_name] = all_titles\n",
    "    print(len(scrapevar['titles'][curr_name]))\n",
    "    scrapevar['sources'][curr_name] = all_sources\n",
    "    scrapevar['pubdates'][curr_name] = all_pubdates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scrapevar for later use\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/Attempt4_mr_scrape.pickle', 'wb') as handle:\n",
    "    pickle.dump(scrapevar, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
