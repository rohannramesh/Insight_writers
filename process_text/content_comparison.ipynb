{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from lxml.html import fromstring\n",
    "from collections import Counter\n",
    "from requests.packages.urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests import Session, exceptions\n",
    "import sys\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from newspaper import Article\n",
    "import json\n",
    "from string import digits\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter \n",
    "from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes word based on it's parts of speech\n",
    "from nltk.corpus import stopwords \n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import cmudict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load from muckrack\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/Attempt3_mr_scrape.pickle', 'rb') as handle:\n",
    "    scrapevar = pickle.load(handle)\n",
    "    \n",
    "# load writer df\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_df.pickle', 'rb') as handle:\n",
    "    writer_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection with database\n",
    "client = MongoClient()\n",
    "mydb = client[\"testinsightdb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "currcol = mydb['zachlowe_nba']\n",
    "y = currcol.find({\"name\": 'zachlowe_nba'})\n",
    "currcol = mydb['timkawakami']\n",
    "t = currcol.find({\"name\": 'timkawakami'})\n",
    "# print(y[0]['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txtstr):\n",
    "    txtstr = re.sub(r'\\n\\s*\\n', '', txtstr) # extra lines\n",
    "    txtstr = re.sub(r'[^\\w\\s]','',txtstr) # punctuation\n",
    "    # for numbers\n",
    "#     txtstr = re.sub(r'[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]', ' ', txtstr)\n",
    "#     txtstr = re.sub(r'\\w*\\d\\w*','',txtstr) # words with number plust letters\n",
    "#     remove_digits = str.maketrans('', '', digits)\n",
    "#     txtstr = txtstr.translate(remove_digits)\n",
    "    txtstr = re.sub(' +',' ',txtstr) # extra white spaces\n",
    "    txtstr = txtstr.lower()\n",
    "    return txtstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer()\n",
    "vec.fit([clean_text(y[0]['article']), clean_text(y[1]['article']), clean_text(t[1]['article'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1415)\n",
      "(1, 1415)\n",
      "(1, 1415)\n"
     ]
    }
   ],
   "source": [
    "vector1 = vec.transform([clean_text(y[0]['article'])])\n",
    "vector2 = vec.transform([clean_text(y[15]['article'])])\n",
    "vector3 = vec.transform([clean_text(t[1]['article'])])\n",
    "print(np.shape(vector1))\n",
    "print(np.shape(vector2))\n",
    "print(np.shape(vector3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89821625]]\n",
      "[[0.5884013]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "result1 = cosine_similarity(vector1, vector2)\n",
    "result2 = cosine_similarity(vector1, vector3)\n",
    "print(result1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"The dog.\",\n",
    "    \"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
