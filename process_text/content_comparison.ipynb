{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from lxml.html import fromstring\n",
    "from collections import Counter\n",
    "from requests.packages.urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests import Session, exceptions\n",
    "import sys\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from newspaper import Article\n",
    "import json\n",
    "from string import digits\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter \n",
    "from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes word based on it's parts of speech\n",
    "from nltk.corpus import stopwords \n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import cmudict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from langdetect import detect\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import rake_nltk\n",
    "sys.path.append(\"/Users/rohanramesh/Documents/GitHub/Insight_writers/Dash_to_server/\")\n",
    "from text_processing import ProcessArticle as pa\n",
    "import suggestions as s\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load from muckrack\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/Attempt3_mr_scrape.pickle', 'rb') as handle:\n",
    "    scrapevar = pickle.load(handle)\n",
    "    \n",
    "# load writer df\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_df.pickle', 'rb') as handle:\n",
    "    writer_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection with Mongo database\n",
    "client = MongoClient()\n",
    "mydb = client[\"testinsightdb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zachlowe_nba\n",
      "adrian-wojnarowski\n",
      "lee-jenkins\n",
      "howardbeck\n",
      "marc-stein\n",
      "ethan-sherwood-strauss\n",
      "jason-concepcion\n",
      "kevin-arnovitz\n",
      "tom-haberstroh\n",
      "nate-duncan\n",
      "zach-harper\n",
      "brian-windhorst\n",
      "sam_amick\n",
      "david-aldridge\n",
      "ramona-shelburne\n",
      "jpdabrams\n",
      "kevin-pelton\n",
      "marc-j-spears\n",
      "matt-moore\n",
      "shams-charania\n",
      "kevin-p-oconnor\n",
      "chris-haynes\n",
      "dave-mcmenamin\n",
      "shea-serrano\n",
      "ian-begley\n",
      "rachel-nichols\n",
      "ben-golliver\n",
      "mike-prada\n",
      "robmahoney\n",
      "tim-macmahon\n",
      "chris-herring\n",
      "billsimmons\n",
      "jonathan-tjarks\n",
      "amin-elhassen\n",
      "bobby-marks\n",
      "danny-leroux\n",
      "chris-mannix\n",
      "dan-devine\n",
      "michael-pina\n",
      "thompsonscribe\n",
      "ben-golliver\n",
      "ben-falk\n",
      "ian-levy\n",
      "tim-bontemps\n",
      "henry-abbott\n",
      "scott-rafferty\n",
      "ja-dubin\n",
      "michael-lee\n",
      "alex-kennedy\n",
      "derek-bodner\n",
      "tom-ziller\n",
      "chris-ballard\n",
      "josh-eberley\n",
      "adi-joseph\n",
      "adam-mares\n",
      "sam-vecenie\n",
      "meet-katie-nolan\n",
      "andrew-sharp\n",
      "david-thorpe\n",
      "royce-webb\n",
      "ericpincus\n",
      "nick-sciria\n",
      "paul-flannery\n",
      "timkawakami\n",
      "keith-smith\n",
      "joshrobbins\n",
      "jeff-zillgitt\n",
      "jon-krawczynski\n",
      "seerat-sohi\n",
      "israel-gutierrez\n",
      "ben-rohrbach\n",
      "mika-honkasalo\n",
      "ian-oconnor-1027184\n",
      "kevin-ferrigan\n",
      "chris-vernon\n",
      "trevor-magnotti\n",
      "mark-deeks\n",
      "ken-berger\n",
      "beckley-mason\n",
      "bob-ryan\n",
      "neil-paine\n",
      "jeff-siegel\n",
      "sam-amico\n",
      "mason-ginsberg\n",
      "britt-robson\n",
      "holly-mackenzie\n",
      "blake-murphy\n",
      "sam-esfandiari\n",
      "jade-hoye\n",
      "michael-grange\n",
      "ric-bucher\n",
      "kevin-parrish\n",
      "jared-zwerling\n",
      "frank-isola\n",
      "kacy-sager\n",
      "austin-hutchinson\n",
      "senthil-natarajan\n",
      "kaileigh-brandt\n",
      "trill-withers\n",
      "krishna-narsu\n",
      "chris-bernucca\n",
      "chris-sheridan\n",
      "scott-cacciola\n",
      "albert-nahmad\n",
      "showardcooper\n",
      "james-holas\n",
      "brady-klopfer\n",
      "josh-eberley\n",
      "rory-masterson\n",
      "grant-afseth\n",
      "rob-lopez\n",
      "justin-willard\n",
      "justin-hodges\n",
      "christian-rivas\n",
      "andrew-bernucca\n",
      "steve-kyler\n",
      "david-morrow\n",
      "austin-green\n",
      "mike-brady\n",
      "peter-vecsey\n",
      "rob-scott\n",
      "paolo-uggetti\n",
      "coley-mick\n",
      "noah-torr\n",
      "george-rowland\n",
      "editor-dan\n",
      "dennis-chambers\n",
      "chris_broussard\n",
      "justin-jett\n",
      "jimmy-spencer\n",
      "tevin-williams\n",
      "ti-windisch\n",
      "glory-okoli\n",
      "aaron-bruski\n",
      "paul-centopani\n",
      "stephen-a-smith\n",
      "andr√©-voigt\n",
      "skip-bayless\n",
      "seb-dumitru\n"
     ]
    }
   ],
   "source": [
    "# to get the number of words per article\n",
    "nwords_article = []\n",
    "for curr_name in writer_df['website_name']:\n",
    "    # point to right collection in database\n",
    "    currcol = mydb[curr_name]\n",
    "    print(curr_name)\n",
    "    # get all articles\n",
    "    y = currcol.find({\"name\": curr_name})\n",
    "    # iterate through all articles for that author\n",
    "    idx = 1\n",
    "    for curr_art in y:\n",
    "        # tokenize words and append to get number of words per article\n",
    "        a = word_tokenize(clean_text(curr_art['article']))\n",
    "        nwords_article.append(len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-IDF\n",
    "Try Tf-IDF as approach for suggesting content. Will also try word2vec for lower dimensional reprensentation. Looks at ratio of word frequency to corpus frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zachlowe_nba\n",
      "breaking\n",
      "adrian-wojnarowski\n",
      "breaking\n",
      "lee-jenkins\n",
      "howardbeck\n",
      "marc-stein\n",
      "ethan-sherwood-strauss\n",
      "jason-concepcion\n",
      "kevin-arnovitz\n",
      "tom-haberstroh\n",
      "nate-duncan\n",
      "zach-harper\n",
      "brian-windhorst\n",
      "sam_amick\n",
      "david-aldridge\n",
      "ramona-shelburne\n",
      "jpdabrams\n",
      "kevin-pelton\n",
      "marc-j-spears\n",
      "matt-moore\n",
      "shams-charania\n",
      "kevin-p-oconnor\n",
      "chris-haynes\n",
      "dave-mcmenamin\n",
      "shea-serrano\n",
      "ian-begley\n",
      "rachel-nichols\n",
      "ben-golliver\n",
      "mike-prada\n",
      "robmahoney\n",
      "tim-macmahon\n",
      "chris-herring\n",
      "billsimmons\n",
      "jonathan-tjarks\n",
      "amin-elhassen\n",
      "bobby-marks\n",
      "danny-leroux\n",
      "chris-mannix\n",
      "dan-devine\n",
      "michael-pina\n",
      "thompsonscribe\n",
      "ben-golliver\n",
      "ben-falk\n",
      "ian-levy\n",
      "tim-bontemps\n",
      "henry-abbott\n",
      "scott-rafferty\n",
      "ja-dubin\n",
      "michael-lee\n",
      "alex-kennedy\n",
      "derek-bodner\n",
      "tom-ziller\n",
      "chris-ballard\n",
      "josh-eberley\n",
      "adi-joseph\n",
      "adam-mares\n",
      "sam-vecenie\n",
      "meet-katie-nolan\n",
      "andrew-sharp\n",
      "david-thorpe\n",
      "royce-webb\n",
      "ericpincus\n",
      "nick-sciria\n",
      "paul-flannery\n",
      "timkawakami\n",
      "keith-smith\n",
      "joshrobbins\n",
      "jeff-zillgitt\n",
      "jon-krawczynski\n",
      "seerat-sohi\n",
      "israel-gutierrez\n",
      "ben-rohrbach\n",
      "mika-honkasalo\n",
      "ian-oconnor-1027184\n",
      "kevin-ferrigan\n",
      "chris-vernon\n",
      "trevor-magnotti\n",
      "mark-deeks\n",
      "ken-berger\n",
      "beckley-mason\n",
      "bob-ryan\n",
      "neil-paine\n",
      "jeff-siegel\n",
      "sam-amico\n",
      "mason-ginsberg\n",
      "britt-robson\n",
      "holly-mackenzie\n",
      "blake-murphy\n",
      "sam-esfandiari\n",
      "jade-hoye\n",
      "michael-grange\n",
      "ric-bucher\n",
      "kevin-parrish\n",
      "jared-zwerling\n",
      "frank-isola\n",
      "kacy-sager\n",
      "austin-hutchinson\n",
      "senthil-natarajan\n",
      "kaileigh-brandt\n",
      "trill-withers\n",
      "krishna-narsu\n",
      "chris-bernucca\n",
      "chris-sheridan\n",
      "scott-cacciola\n",
      "albert-nahmad\n",
      "showardcooper\n",
      "james-holas\n",
      "brady-klopfer\n",
      "josh-eberley\n",
      "rory-masterson\n",
      "grant-afseth\n",
      "rob-lopez\n",
      "justin-willard\n",
      "justin-hodges\n",
      "christian-rivas\n",
      "andrew-bernucca\n",
      "steve-kyler\n",
      "david-morrow\n",
      "austin-green\n",
      "mike-brady\n",
      "peter-vecsey\n",
      "rob-scott\n",
      "paolo-uggetti\n",
      "coley-mick\n",
      "noah-torr\n",
      "george-rowland\n",
      "editor-dan\n",
      "dennis-chambers\n",
      "chris_broussard\n",
      "justin-jett\n",
      "jimmy-spencer\n",
      "tevin-williams\n",
      "ti-windisch\n",
      "glory-okoli\n",
      "aaron-bruski\n",
      "paul-centopani\n",
      "stephen-a-smith\n",
      "andr√©-voigt\n",
      "skip-bayless\n",
      "seb-dumitru\n"
     ]
    }
   ],
   "source": [
    "# gather and preprocess data for training tf-idf model\n",
    "data_into_tfidf = []\n",
    "# iterate over each name in the database\n",
    "for curr_name in writer_df['website_name']:\n",
    "    # current collection\n",
    "    currcol = mydb[curr_name]\n",
    "    print(curr_name)\n",
    "    # all articles in the collection\n",
    "    y = currcol.find({\"name\": curr_name})\n",
    "    # iterate through all articles for that author\n",
    "    idx = 1\n",
    "    for curr_art in y:\n",
    "        # do preprocessing of text using process articles class\n",
    "        curr = pa(curr_art['article'])\n",
    "        curr.clean_text()\n",
    "        data_into_tfidf.append(clean_text(curr.cleanedtext))\n",
    "        # don't inlude more than 250 articles from any one author\n",
    "        idx += 1\n",
    "        if idx > 250:\n",
    "            print('breaking')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train tf-idf vectorizer\n",
    "vec = TfidfVectorizer()\n",
    "vec.fit(data_into_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector of each article and put in list\n",
    "output_vec_tf = []\n",
    "for curr_art in data_into_tfidf:\n",
    "    output_vec_tf.append(vec.transform([curr_art]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "Try Word2Vec as an approach as well for each article. Shallow neural network that will look at context of word to predict the actual word (using continuous bag of words approach for now). Will represent each word as a n dimensional vector (lower dimension than tf-idf) and will average across vectors to get \"content\" of article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zachlowe_nba\n",
      "adrian-wojnarowski\n",
      "lee-jenkins\n",
      "howardbeck\n",
      "marc-stein\n",
      "ethan-sherwood-strauss\n",
      "jason-concepcion\n",
      "kevin-arnovitz\n",
      "tom-haberstroh\n",
      "nate-duncan\n",
      "zach-harper\n",
      "brian-windhorst\n",
      "sam_amick\n",
      "david-aldridge\n",
      "ramona-shelburne\n",
      "jpdabrams\n",
      "kevin-pelton\n",
      "marc-j-spears\n",
      "matt-moore\n",
      "shams-charania\n",
      "kevin-p-oconnor\n",
      "chris-haynes\n",
      "dave-mcmenamin\n",
      "shea-serrano\n",
      "ian-begley\n",
      "rachel-nichols\n",
      "ben-golliver\n",
      "mike-prada\n",
      "robmahoney\n",
      "tim-macmahon\n",
      "chris-herring\n",
      "billsimmons\n",
      "jonathan-tjarks\n",
      "amin-elhassen\n",
      "bobby-marks\n",
      "danny-leroux\n",
      "chris-mannix\n",
      "dan-devine\n",
      "michael-pina\n",
      "thompsonscribe\n",
      "ben-golliver\n",
      "ben-falk\n",
      "ian-levy\n",
      "tim-bontemps\n",
      "henry-abbott\n",
      "scott-rafferty\n",
      "ja-dubin\n",
      "michael-lee\n",
      "alex-kennedy\n",
      "derek-bodner\n",
      "tom-ziller\n",
      "chris-ballard\n",
      "josh-eberley\n",
      "adi-joseph\n",
      "adam-mares\n",
      "sam-vecenie\n",
      "meet-katie-nolan\n",
      "andrew-sharp\n",
      "david-thorpe\n",
      "royce-webb\n",
      "ericpincus\n",
      "nick-sciria\n",
      "paul-flannery\n",
      "timkawakami\n",
      "keith-smith\n",
      "joshrobbins\n",
      "jeff-zillgitt\n",
      "jon-krawczynski\n",
      "seerat-sohi\n",
      "israel-gutierrez\n",
      "ben-rohrbach\n",
      "mika-honkasalo\n",
      "ian-oconnor-1027184\n",
      "kevin-ferrigan\n",
      "chris-vernon\n",
      "trevor-magnotti\n",
      "mark-deeks\n",
      "ken-berger\n",
      "beckley-mason\n",
      "bob-ryan\n",
      "neil-paine\n",
      "jeff-siegel\n",
      "sam-amico\n",
      "mason-ginsberg\n",
      "britt-robson\n",
      "holly-mackenzie\n",
      "blake-murphy\n",
      "sam-esfandiari\n",
      "jade-hoye\n",
      "michael-grange\n",
      "ric-bucher\n",
      "kevin-parrish\n",
      "jared-zwerling\n",
      "frank-isola\n",
      "kacy-sager\n",
      "austin-hutchinson\n",
      "senthil-natarajan\n",
      "kaileigh-brandt\n",
      "trill-withers\n",
      "krishna-narsu\n",
      "chris-bernucca\n",
      "chris-sheridan\n",
      "scott-cacciola\n",
      "albert-nahmad\n",
      "showardcooper\n",
      "james-holas\n",
      "brady-klopfer\n",
      "josh-eberley\n",
      "rory-masterson\n",
      "grant-afseth\n",
      "rob-lopez\n",
      "justin-willard\n",
      "justin-hodges\n",
      "christian-rivas\n",
      "andrew-bernucca\n",
      "steve-kyler\n",
      "david-morrow\n",
      "austin-green\n",
      "mike-brady\n",
      "peter-vecsey\n",
      "rob-scott\n",
      "paolo-uggetti\n",
      "coley-mick\n",
      "noah-torr\n",
      "george-rowland\n",
      "editor-dan\n",
      "dennis-chambers\n",
      "chris_broussard\n",
      "justin-jett\n",
      "jimmy-spencer\n",
      "tevin-williams\n",
      "ti-windisch\n",
      "glory-okoli\n",
      "aaron-bruski\n",
      "paul-centopani\n",
      "stephen-a-smith\n",
      "andr√©-voigt\n",
      "skip-bayless\n",
      "seb-dumitru\n"
     ]
    }
   ],
   "source": [
    "# build corpus for training word2vec model\n",
    "# saving three values - list containing preprocessed text for each article, \n",
    "# url information for each article\n",
    "article_string = []\n",
    "url_string = []\n",
    "# data_into_tfidf = [] # if saving for tf-idf\n",
    "# iterate through each name\n",
    "for curr_name in writer_df['website_name']:\n",
    "    # identify appropriate collection\n",
    "    currcol = mydb[curr_name]\n",
    "    print(curr_name)\n",
    "    # get all articles for that writer\n",
    "    y = currcol.find({\"name\": curr_name})\n",
    "    for curr_article in y:\n",
    "        # check if article is english and if not then continue\n",
    "        a = s.detect_nonenglish(curr_article['article'])\n",
    "        if a == True:\n",
    "            continue\n",
    "        # for word2vec\n",
    "        lem_art = s.lemstr(gensim.utils.simple_preprocess(curr_article['article']))\n",
    "        article_string.append(lem_art)\n",
    "        # if building for tf-idf\n",
    "#         data_into_tfidf.append(curr.cleanedtext)\n",
    "        url_string.append(curr_article['article_url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112255268, 146738100)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try word2vec model\n",
    "# vector length for each word\n",
    "vec_size = 150\n",
    "model = gensim.models.Word2Vec(\n",
    "        article_string,\n",
    "        size=vec_size,\n",
    "        window=12, # total number of words to account for\n",
    "        min_count=2,\n",
    "        workers=8)\n",
    "model.train(article_string, total_examples=len(article_string), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get keyed vectors saved or loaded for faster calculations\n",
    "# model.save(\"/Users/rohanramesh/Documents/Insight/data_bball_writers/word2vec_model.model\")\n",
    "# model.wv.save(\"/Users/rohanramesh/Documents/Insight/data_bball_writers/word2vec_model_kv.kv\")\n",
    "kv = gensim.models.KeyedVectors.load(\n",
    "    \"/Users/rohanramesh/Documents/Insight/data_bball_writers/word2vec_model_kv.kv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each article lets build a vector that corresponds to the entire article by taking the mean\n",
    "# across all words included in the article\n",
    "article_mat = np.ndarray(shape=[np.shape(article_string)[0],vec_size])\n",
    "# iterate over each article\n",
    "for idx in range(0,len(article_string)):\n",
    "    # check if words are in vocab and if not then don't include those words\n",
    "    a = [r for r in article_string[idx] if r in model.wv.vocab]\n",
    "    # build matrix for each word where each row is a word and columns = word embedding vector\n",
    "    array_for_art = np.ndarray(shape=[len(a),vec_size])\n",
    "    for i in range(0,len(a)):\n",
    "        array_for_art[i,:] = model.wv.get_vector(a[i])\n",
    "    # take mean across all words to get the vector for the article\n",
    "    article_mat[idx,:] = np.mean(array_for_art, axis=0)\n",
    "\n",
    "# build similarity matrix across all articles using cosine similarity\n",
    "all_art_cos_sim = np.ndarray(shape=[len(article_mat),len(article_mat)])\n",
    "for i in range(0,len(article_mat)):\n",
    "    for j in range(0,len(article_mat)):\n",
    "        # calculate cos sim between all articles\n",
    "        all_art_cos_sim[i,j] = s.cos_sim(article_mat[i,:],article_mat[j,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataframe with word2vec dim reduced vectors\n",
    "w2v_df = pd.DataFrame()\n",
    "for i in range(0,len(article_mat)):\n",
    "    w2v_df[url_string[i]] = article_mat[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save w2v as pickle for loading\n",
    "w2v_df.to_pickle('/Users/rohanramesh/Documents/Insight/data_bball_writers/word2vec_trained.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "Try Doc2Vec as an approach as well for each article. Apparently tuning takes a bit more work with doc2vec so might need to sink more time to get this to work as well as word2vec. Have to reformat input to model to accomade for \"tagged\" documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying Doc2vec - have to load in data plus tags\n",
    "docs = []\n",
    "# iterate through each writer and collection to get each article\n",
    "for curr_name in writer_df['website_name']:\n",
    "    currcol = mydb[curr_name]\n",
    "    print(curr_name)\n",
    "    y = currcol.find({\"name\": curr_name})\n",
    "    for curr_article in y:\n",
    "        # check if english then lemmatize and append for doc structure in doc2vec\n",
    "        a = s.detect_nonenglish(curr_article['article'])\n",
    "        if a == True:\n",
    "            continue\n",
    "        lem_art = s.lemstr(gensim.utils.simple_preprocess(curr_article['article']))\n",
    "        docs.append(gensim.models.doc2vec.TaggedDocument(lem_art, curr_article['article_url']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into proper doc2vec formatting using their TaggedDocument approach\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/insight_new/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "# build doc2vec model and vocabulary\n",
    "model = gensim.models.Doc2Vec(size=200, window=10, min_count=2, \n",
    "                              workers=4,alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "model.build_vocab(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with decreasing learning rate\n",
    "for epoch in range(10):\n",
    "    model.train(docs, total_examples=model.corpus_count, epochs=1)\n",
    "    model.alpha -= 0.002 # decrease the learning rate\n",
    "    model.min_alpha = model.alpha # fix the learning rate, no deca\n",
    "    model.train(docs, total_examples=model.corpus_count, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector representations of each article using Doc2vec using same approach as for word2vec\n",
    "article_mat = np.ndarray(shape=[np.shape(docs)[0], model.vector_size])\n",
    "for idx in range(0,np.shape(docs)[0]):\n",
    "    # make sure in vocab\n",
    "    a = [r for r in docs[idx][0] if r in model.wv.vocab]\n",
    "    # average across all words\n",
    "    array_for_art = np.ndarray(shape=[len(a), model.vector_size])\n",
    "    for i in range(0,len(a)):\n",
    "        array_for_art[i,:] = model.wv.get_vector(a[i])\n",
    "    article_mat[idx,:] = np.mean(array_for_art, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataframe with doc2vec dim reduced vectors and save\n",
    "d2v_df = pd.DataFrame()\n",
    "for i in range(0,len(article_mat)):\n",
    "    d2v_df[docs[i][1]] = article_mat[i,:]\n",
    "    \n",
    "# save doc2vec model and keyedvectors\n",
    "model.save(\"/Users/rohanramesh/Documents/Insight/data_bball_writers/doc2vec_model.model\")\n",
    "model.wv.save(\"/Users/rohanramesh/Documents/Insight/data_bball_writers/doc2vec_model_kv.kv\")\n",
    "d2v_df.to_pickle(\"/Users/rohanramesh/Documents/Insight/data_bball_writers/d2v_df.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
