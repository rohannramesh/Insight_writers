{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "from lxml.html import fromstring\n",
    "from collections import Counter\n",
    "from requests.packages.urllib3.util import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests import Session, exceptions\n",
    "import sys\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from newspaper import Article\n",
    "import json\n",
    "from string import digits\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter \n",
    "from nltk.corpus import wordnet # To get words in dictionary with their parts of speech\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes word based on it's parts of speech\n",
    "from nltk.corpus import stopwords \n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import cmudict\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load from muckrack\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/Attempt3_mr_scrape.pickle', 'rb') as handle:\n",
    "    scrapevar = pickle.load(handle)\n",
    "    \n",
    "# load writer df\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_df.pickle', 'rb') as handle:\n",
    "    writer_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection with database\n",
    "client = MongoClient()\n",
    "mydb = client[\"testinsightdb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "currcol = mydb['zachlowe_nba']\n",
    "y = currcol.find({\"name\": 'zachlowe_nba'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "def clean_text(txtstr):\n",
    "    txtstr = re.sub(r'\\n\\s*\\n', '', txtstr) # extra lines\n",
    "    txtstr = re.sub(r'[^\\w\\s]','',txtstr) # punctuation\n",
    "    # for numbers\n",
    "#     txtstr = re.sub(r'[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]', ' ', txtstr)\n",
    "#     txtstr = re.sub(r'\\w*\\d\\w*','',txtstr) # words with number plust letters\n",
    "#     remove_digits = str.maketrans('', '', digits)\n",
    "#     txtstr = txtstr.translate(remove_digits)\n",
    "    txtstr = re.sub(' +',' ',txtstr) # extra white spaces\n",
    "    txtstr = txtstr.lower()\n",
    "    return txtstr\n",
    "\n",
    "def clean_text_no_numbers(txtstr):\n",
    "    txtstr = re.sub(r'\\n\\s*\\n', '', txtstr) # extra lines\n",
    "    txtstr = re.sub(r'[^\\w\\s]','',txtstr) # punctuation\n",
    "    # for numbers\n",
    "    txtstr = re.sub(r'[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]', ' ', txtstr)\n",
    "    txtstr = re.sub(r'\\w*\\d\\w*','',txtstr) # words with number plust letters\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    txtstr = txtstr.translate(remove_digits)\n",
    "    txtstr = re.sub(' +',' ',txtstr) # extra white spaces\n",
    "    txtstr = txtstr.lower()\n",
    "    return txtstr\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(word_tokens): # must be tokenized sentence\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return filtered_sentence\n",
    "\n",
    "# stem long string - must be tokenized\n",
    "def stemstr(txtstr):\n",
    "    ps = PorterStemmer()\n",
    "    newstr = [ps.stem(curr_word) for curr_word in txtstr]\n",
    "    return newstr\n",
    "\n",
    "# for figuring out what type of word\n",
    "def get_pos( word ):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in w_synsets if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in w_synsets if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in w_synsets if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in w_synsets if item.pos()==\"r\"]  )\n",
    "    \n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0] # first indexer for \n",
    "            # getting the top POS from list, second indexer for getting POS from tuple( POS: count )\n",
    "\n",
    "# lemmatize long string - must be tokenized\n",
    "def lemstr(txtstr):\n",
    "    lz = WordNetLemmatizer()\n",
    "    newstr = [lz.lemmatize(curr_word, get_pos(curr_word)) for curr_word in txtstr]\n",
    "    return newstr\n",
    "\n",
    "# get number of words\n",
    "def get_nwords(txtstr):\n",
    "    b = word_tokenize(txtstr)\n",
    "    n_words = len(b)\n",
    "    return n_words\n",
    "\n",
    "# get number of sentences and nwords per sentences and sentence length variability\n",
    "def get_nsentences(txtstr):\n",
    "    b = sent_tokenize(txtstr)\n",
    "    n_sentences = len(b)\n",
    "    nwords = []\n",
    "    for curr_sent in b:\n",
    "        nwords.append(get_nwords(curr_sent))\n",
    "    return n_sentences, np.mean(nwords), np.std(nwords)\n",
    "\n",
    "\n",
    "# nsyll of a word\n",
    "def nsyl(word):\n",
    "    d = cmudict.dict()\n",
    "    output = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]]\n",
    "    return output\n",
    "\n",
    "\n",
    "# n syllables of longer string\n",
    "def n_syllables(txtstr):\n",
    "    b = word_tokenize(txtstr)\n",
    "    syll_each_word = []\n",
    "    for curr_word in b:\n",
    "        syll_each_word.append(nsyl(curr_word)[0])\n",
    "    return syll_each_word\n",
    "        \n",
    "\n",
    "# get flesch kincaid value\n",
    "def get_fk_value(txtstr):\n",
    "    # get n sentences\n",
    "    a = word_tokenize(txtstr)\n",
    "    n_words = len(a)\n",
    "    # get n words\n",
    "    b = sent_tokenize(txtstr)\n",
    "    n_sentences = len(b)    \n",
    "    score = 206.835 - 1.015 * (n_words / n_sentences) - 84.6 * (num_syllables / n_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "10\n",
      "33\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "curr_name = 'marc-stein'\n",
    "currcol = mydb[curr_name]\n",
    "y = currcol.find({\"name\": curr_name})\n",
    "curr_art = y[0]\n",
    "cleantext = clean_text(curr_art['article'])\n",
    "nostopwords = remove_stopwords(word_tokenize(cleantext))\n",
    "typesofspeech = [get_pos(i) for i in nostopwords]\n",
    "curr_art = y[0]\n",
    "# print(typesofspeech)\n",
    "print(typesofspeech.count('n'))\n",
    "print(typesofspeech.count('a'))\n",
    "print(typesofspeech.count('v'))\n",
    "print(typesofspeech.count('r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zachlowe_nba\n",
      "34.60597562789917\n",
      "adrian-wojnarowski\n",
      "6.089301824569702\n",
      "lee-jenkins\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b9bbdfcd161d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mn_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_nwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleantext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# n sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnw_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnw_s_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nsentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_art\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mn_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mn_wordspersentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnw_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-bb77400fc3aa>\u001b[0m in \u001b[0;36mget_nsentences\u001b[0;34m(txtstr)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mnwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcurr_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mnwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_nwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-bb77400fc3aa>\u001b[0m in \u001b[0;36mget_nwords\u001b[0;34m(txtstr)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# get number of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_nwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxtstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxtstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mn_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/insight_new/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/anaconda3/envs/insight_new/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/insight_new/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# add some basic features for each article\n",
    "mongo_id = []\n",
    "n_words = []\n",
    "n_sentences = []\n",
    "n_wordspersentence = []\n",
    "n_word_sent_var = []\n",
    "author_list = []\n",
    "author_id_n = []\n",
    "n_nouns = []\n",
    "n_adj = []\n",
    "n_verb = []\n",
    "n_adv =[]\n",
    "# for sentiment analysis\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "avg_sentim = []\n",
    "sentim_var = []\n",
    "# for more specific pos\n",
    "# adj, comp adj, super adj, noun, proper sing n, proper pl n, \n",
    "# plural n, adv, comp adv, super avd\n",
    "pos_test = ['JJ', 'JJR', 'JJS', 'NN', 'NNP', 'NNPS', 'NNS', 'RB', 'RBR', 'RBS']\n",
    "extra_pos = {}\n",
    "for curr_pos in pos_test:\n",
    "    extra_pos[curr_pos] = []\n",
    "\n",
    "# iterate over all writers\n",
    "for curr_name in writer_df['website_name']:\n",
    "    print(curr_name)\n",
    "    t = time.time()\n",
    "    currcol = mydb[curr_name]\n",
    "    y = currcol.find({\"name\": curr_name})\n",
    "    # iterate through all articles for that author\n",
    "    for curr_art in y:\n",
    "        # mongo id number\n",
    "        mongo_id.append(str(curr_art['_id']))\n",
    "        # n words after cleaning and removing numbers and punctuation\n",
    "        cleantext = clean_text(curr_art['article'])\n",
    "        n_words.append(get_nwords(cleantext))\n",
    "        # n sentences\n",
    "        ns, nw_s, nw_s_var = get_nsentences(curr_art['article'])\n",
    "        n_sentences.append(ns)\n",
    "        n_wordspersentence.append(nw_s)\n",
    "        n_word_sent_var.append(nw_s_var)\n",
    "        # author list\n",
    "        author_list.append(curr_name)\n",
    "        # author id n\n",
    "        author_id_n.append(curr_art['writer_id'])\n",
    "        # remove stopwords from story\n",
    "        nostopwords = remove_stopwords(word_tokenize(cleantext))\n",
    "        typesofspeech = [get_pos(i) for i in nostopwords]\n",
    "        n_nouns.append(typesofspeech.count('n'))\n",
    "        n_adj.append(typesofspeech.count('a'))\n",
    "        n_verb.append(typesofspeech.count('v'))\n",
    "        n_adv.append(typesofspeech.count('r'))\n",
    "        sentim_all = {}\n",
    "        sentim_all['neg'] = []\n",
    "        sentim_all['neu'] = []\n",
    "        sentim_all['pos'] = []\n",
    "        sentim_all['compound'] = []\n",
    "        sentences = sent_tokenize(curr_art['article'])\n",
    "        for i in sentences:\n",
    "            vs = analyzer.polarity_scores(i)\n",
    "            sentim_all['neg'].append(vs['neg'])\n",
    "            sentim_all['neu'].append(vs['neu'])\n",
    "            sentim_all['pos'].append(vs['pos'])\n",
    "            sentim_all['compound'].append(vs['compound'])\n",
    "        # take avg sentim for each story by averaging sentim for each sentence\n",
    "        avg_sentim.append([np.mean(sentim_all['neg']), np.mean(sentim_all['neu']), \n",
    "                         np.mean(sentim_all['pos']), np.mean(sentim_all['compound'])])\n",
    "        # take std of sentim for each story to measure sentiment variability\n",
    "        sentim_var.append([np.std(sentim_all['neg']), np.std(sentim_all['neu']), \n",
    "                         np.std(sentim_all['pos']), np.std(sentim_all['compound'])])\n",
    "    elapsed = time.time() - t\n",
    "    print(elapsed)\n",
    "#         # for more specific pos\n",
    "#         doc = nlp(curr_art['article'])\n",
    "#         pos_vec = [token.tag_ for token in doc]        \n",
    "#         for curr_pos in pos_test:\n",
    "#             extra_pos[curr_pos].append(pos_vec.count(curr_pos))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/insight_new/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/envs/insight_new/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/anaconda3/envs/insight_new/lib/python3.6/site-packages/numpy/core/_methods.py:135: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/anaconda3/envs/insight_new/lib/python3.6/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/anaconda3/envs/insight_new/lib/python3.6/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "avg_sentim = []\n",
    "sentim_var = []\n",
    "for curr_name in writer_df['website_name']:\n",
    "    currcol = mydb[curr_name]\n",
    "    y = currcol.find({\"name\": curr_name})\n",
    "    # iterate through all articles for that author\n",
    "    for curr_art in y:\n",
    "        sentim_all = {}\n",
    "        sentim_all['neg'] = []\n",
    "        sentim_all['neu'] = []\n",
    "        sentim_all['pos'] = []\n",
    "        sentim_all['compound'] = []\n",
    "        sentences = sent_tokenize(curr_art['article'])\n",
    "        for i in sentences:\n",
    "        #             a = clean_text(i)\n",
    "            vs = analyzer.polarity_scores(i)\n",
    "            sentim_all['neg'].append(vs['neg'])\n",
    "            sentim_all['neu'].append(vs['neu'])\n",
    "            sentim_all['pos'].append(vs['pos'])\n",
    "            sentim_all['compound'].append(vs['compound'])\n",
    "        # take avg sentim for each story by averaging sentim for each sentence\n",
    "        avg_sentim.append([np.mean(sentim_all['neg']), np.mean(sentim_all['neu']), \n",
    "                         np.mean(sentim_all['pos']), np.mean(sentim_all['compound'])])\n",
    "        # take std of sentim for each story to measure sentiment variability\n",
    "        sentim_var.append([np.std(sentim_all['neg']), np.std(sentim_all['neu']), \n",
    "                         np.std(sentim_all['pos']), np.std(sentim_all['compound'])])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "      <th>author_id_n</th>\n",
       "      <th>author_list</th>\n",
       "      <th>mongo_id</th>\n",
       "      <th>n_nouns</th>\n",
       "      <th>n_adj</th>\n",
       "      <th>n_verb</th>\n",
       "      <th>n_adv</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_wordspersentence</th>\n",
       "      <th>n_wordspersent_variability</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>sentiment_variability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2966</td>\n",
       "      <td>693716</td>\n",
       "      <td>zachlowe_nba</td>\n",
       "      <td>5b9c705c11919a2b1a15c30d</td>\n",
       "      <td>1245</td>\n",
       "      <td>67</td>\n",
       "      <td>413</td>\n",
       "      <td>102</td>\n",
       "      <td>185</td>\n",
       "      <td>19.854054</td>\n",
       "      <td>17.212670</td>\n",
       "      <td>[0.0781081081081081, 0.839637837837838, 0.0822...</td>\n",
       "      <td>[0.1340543221444573, 0.18581430333818436, 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2966</td>\n",
       "      <td>693716</td>\n",
       "      <td>zachlowe_nba</td>\n",
       "      <td>5b9c705d11919a2b1a15c30e</td>\n",
       "      <td>1245</td>\n",
       "      <td>67</td>\n",
       "      <td>413</td>\n",
       "      <td>102</td>\n",
       "      <td>185</td>\n",
       "      <td>19.854054</td>\n",
       "      <td>17.212670</td>\n",
       "      <td>[0.0781081081081081, 0.839637837837838, 0.0822...</td>\n",
       "      <td>[0.1340543221444573, 0.18581430333818436, 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2697</td>\n",
       "      <td>693716</td>\n",
       "      <td>zachlowe_nba</td>\n",
       "      <td>5b9c705f11919a2b1a15c30f</td>\n",
       "      <td>1000</td>\n",
       "      <td>60</td>\n",
       "      <td>363</td>\n",
       "      <td>112</td>\n",
       "      <td>151</td>\n",
       "      <td>20.774834</td>\n",
       "      <td>16.185666</td>\n",
       "      <td>[0.03782781456953642, 0.8699602649006623, 0.09...</td>\n",
       "      <td>[0.0837185256931255, 0.13923004360998542, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2813</td>\n",
       "      <td>693716</td>\n",
       "      <td>zachlowe_nba</td>\n",
       "      <td>5b9c706011919a2b1a15c310</td>\n",
       "      <td>1060</td>\n",
       "      <td>58</td>\n",
       "      <td>411</td>\n",
       "      <td>86</td>\n",
       "      <td>183</td>\n",
       "      <td>18.568306</td>\n",
       "      <td>11.751734</td>\n",
       "      <td>[0.05924590163934427, 0.854087431693989, 0.086...</td>\n",
       "      <td>[0.10656453275679194, 0.14342395028086777, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2813</td>\n",
       "      <td>693716</td>\n",
       "      <td>zachlowe_nba</td>\n",
       "      <td>5b9c706111919a2b1a15c311</td>\n",
       "      <td>1060</td>\n",
       "      <td>58</td>\n",
       "      <td>411</td>\n",
       "      <td>86</td>\n",
       "      <td>183</td>\n",
       "      <td>18.568306</td>\n",
       "      <td>11.751734</td>\n",
       "      <td>[0.05924590163934427, 0.854087431693989, 0.086...</td>\n",
       "      <td>[0.10656453275679194, 0.14342395028086777, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_words  author_id_n   author_list                  mongo_id  n_nouns  \\\n",
       "0     2966       693716  zachlowe_nba  5b9c705c11919a2b1a15c30d     1245   \n",
       "1     2966       693716  zachlowe_nba  5b9c705d11919a2b1a15c30e     1245   \n",
       "2     2697       693716  zachlowe_nba  5b9c705f11919a2b1a15c30f     1000   \n",
       "3     2813       693716  zachlowe_nba  5b9c706011919a2b1a15c310     1060   \n",
       "4     2813       693716  zachlowe_nba  5b9c706111919a2b1a15c311     1060   \n",
       "\n",
       "   n_adj  n_verb  n_adv  n_sentences  n_wordspersentence  \\\n",
       "0     67     413    102          185           19.854054   \n",
       "1     67     413    102          185           19.854054   \n",
       "2     60     363    112          151           20.774834   \n",
       "3     58     411     86          183           18.568306   \n",
       "4     58     411     86          183           18.568306   \n",
       "\n",
       "   n_wordspersent_variability  \\\n",
       "0                   17.212670   \n",
       "1                   17.212670   \n",
       "2                   16.185666   \n",
       "3                   11.751734   \n",
       "4                   11.751734   \n",
       "\n",
       "                                       avg_sentiment  \\\n",
       "0  [0.0781081081081081, 0.839637837837838, 0.0822...   \n",
       "1  [0.0781081081081081, 0.839637837837838, 0.0822...   \n",
       "2  [0.03782781456953642, 0.8699602649006623, 0.09...   \n",
       "3  [0.05924590163934427, 0.854087431693989, 0.086...   \n",
       "4  [0.05924590163934427, 0.854087431693989, 0.086...   \n",
       "\n",
       "                               sentiment_variability  \n",
       "0  [0.1340543221444573, 0.18581430333818436, 0.14...  \n",
       "1  [0.1340543221444573, 0.18581430333818436, 0.14...  \n",
       "2  [0.0837185256931255, 0.13923004360998542, 0.12...  \n",
       "3  [0.10656453275679194, 0.14342395028086777, 0.1...  \n",
       "4  [0.10656453275679194, 0.14342395028086777, 0.1...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a pd of what we have so far\n",
    "data = {'n_words': n_words, 'author_id_n': author_id_n, 'author_list': author_list, 'mongo_id': mongo_id,\n",
    "        'n_nouns': n_nouns, 'n_adj': n_adj, 'n_verb': n_verb, 'n_adv': n_adv, 'n_sentences': n_sentences, \n",
    "        'n_wordspersentence': n_wordspersentence, 'n_wordspersent_variability': n_word_sent_var, \n",
    "        'avg_sentiment': avg_sentim, 'sentiment_variability': sentim_var}\n",
    "writer_feature_df = pd.DataFrame.from_dict(data)\n",
    "writer_feature_df.head()\n",
    "# print(len(n_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://rohanramesh@localhost/writer_feature_db\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# write to postgresql database for later pulling\n",
    "dbname = 'writer_feature_db'\n",
    "username = 'rohanramesh' # change this to your username\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "writer_feature_df.to_sql('writer_feature', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to make queries using psycopg2\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query:\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM writer_feature WHERE author_list ='timkawakami';\n",
    "\"\"\"\n",
    "writer_feature_subsection = pd.read_sql_query(sql_query,con)\n",
    "writer_feature_subsection.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save writer feature df\n",
    "with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_feature_df_2.pickle', 'wb') as handle:\n",
    "    pickle.dump(writer_feature_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load\n",
    "# with open('/Users/rohanramesh/Documents/Insight/data_bball_writers/writer_feature_df_2.pickle', 'rb') as handle:\n",
    "#     writer_feature_df = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
